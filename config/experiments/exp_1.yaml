# @package _global_
info:
  exp_name: "exp_1"
  debug_mode: False

path:
  base: "./data"
  train_file: "dataset.parquet"
  logger: "./logging"
  pretrained_weights: False
  pretrained_file: "./logging/exp_11/model.pt"

dataset: 
  loader_train: 'src.data.dataset.Dataset_Train'
  loader_valid: 'src.data.dataset.Dataset_Train'
  augmentation: 'src.data.augmentation.aug_func'
  preprocessing: 'src.utils.utils.preprocessing_32'
  fold: 'train'
  limit_by_length: False

train:
  seed: 2022
  fp16: False
  backbone: None
  batch_size_train: 64 # 64
  batch_size_valid: 32 # 8 1 shuffle False
  teacher_forcing_ratio: 1
  loss: 'src.metrics.label_smoothing.LabelSmoothing'
  label_smoothing: 0.1
  metric: 'src.metrics.fscore.get_metrics'
  scheduler: False
  epochs: 250
  early_stop_patience: 30
  reduce_lr_factor: 0.25
  reduce_lr_patience: 7
  reduce_lr_min: 1e-6
  num_workers: 3
  lr: 0
  warmup: 200
  weigth_decay: 1e-2
  beta1: 0.9
  beta2: 0.95

model:
  model_name: 'src.models.transformer.GPT'
  input_dim: 32
  cnn_1_dim: 48
  cnn_1_kernel_size: 16
  cnn_1_stride: 1
  n_layer: 4
  n_head: 4
  n_embd: 48
  block_size: 17
  dropout: 0.5
  vocab_size: 10
  bias: False