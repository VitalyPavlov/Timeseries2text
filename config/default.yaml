hydra:
  run:
    dir: "/Users/vitaly/Documents/Github_personal/Timeseries2text"
  output_subdir: null
  job_logging:
    disable_existing_loggers: true
    formatters:
      simple:
        format: '%(message)s'
    console:
      level: 'CRITICAL'
    handlers:
      file:
        filename: '${path.logger}/${info.exp_name}/train.log'
        level: 'DEBUG'
        mode: 'w'

info:
  exp_name: "exp_0"
  debug_mode: False

path:
  base: "./data"
  train_file: "dataset.parquet"
  logger: "./logging"
  pretrained_weights: False
  pretrained_file: "./logging/exp_11/model.pt"
  weights: '${path.logger}/${info.exp_name}/${info.exp_name}_${dataset.fold}.pt'

dataset: 
  loader_train: 'src.data.dataset.Dataset_Train'
  loader_valid: 'src.data.dataset.Dataset_Train'
  augmentation: 'src.data.augmentation.aug_func'
  preprocessing: 'src.utils.utils.preprocessing_32'
  fold: 'train'
  limit_by_length: False

train:
  seed: 2022
  fp16: False
  backbone: None
  batch_size_train: 64 # 64
  batch_size_valid: 32 # 8 1 shuffle False
  teacher_forcing_ratio: 1
  loss: 'torch.nn.CrossEntropyLoss' # 'torch.nn.CrossEntropyLoss' 'src.metrics.focal_loss.FocalLoss'
  label_smoothing: 0
  metric: 'src.metrics.fscore.get_metrics'
  scheduler: "torch.optim.lr_scheduler.ReduceLROnPlateau"
  epochs: 250
  lr: 1e-3 # 2
  early_stop_patience: 30
  reduce_lr_factor: 0.25
  reduce_lr_patience: 7
  reduce_lr_min: 1e-6
  num_workers: 3
